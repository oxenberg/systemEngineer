import re
from collections import Counter,defaultdict
import random
import math

class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns amodel from a given text.
        It supoprts language generation and the evaluation of a given string.
        The class can be applied on both word level and caracter level.
    """

    def __init__(self, n=3, chars=False):
        """Initializing a language model object.
        Arges:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.n_grams = n
        self.split_by_char = chars
        self.total_words_in_corpos = 0
        self.model =  Counter()
        self.model_n_min_1 =  Counter()
        self.sorted_model = {}
        self.join_note = " "
        # self.special_word = "<S>"
        self.oov = True
        self.generate_bool = True
    def build_model(self, text):  #should be called build_model
        """populates a dictionary counting all ngrams in the specified text.
        
            Args:
                text (str): the text to construct the model from.
        """
   
        
        if self.split_by_char:
            list_of_words = list(text)
            self.join_note = "#@"
        else:
            list_of_words = normalize_text(text).split(" ")
            list_of_words = list_of_words[:-1]
        
        
               
        self.total_words_in_corpos = len(list_of_words) #: for normalization
        
        #: add <s> in the begging for calculate window easy
        # self.add_special_char(list_of_words)
        
        if self.n_grams == 1: #: easy case uni-gram
            self.model = Counter(list_of_words)
        else:
            for first_window_index in range(len(list_of_words) - self.n_grams+1):
                word_window = list_of_words[first_window_index:first_window_index+self.n_grams]
                string_window = self.join_note.join(word_window)
                self.model[string_window] += 1
            for first_window_index in range(len(list_of_words) - (self.n_grams)):
                word_window = list_of_words[first_window_index:first_window_index+(self.n_grams-1)]
                string_window = self.join_note.join(word_window)
                self.model_n_min_1[string_window] += 1
        
        
        
        #: i need to create dict witout the <s> sign to get "same model as output"
        # self.model_without_special_word = defaultdict(int)
        
        # for key,value in self.model.items():
        #     if self.special_word not in key:
        #         self.model_without_special_word[key] = value
        
        self.model_defultdict = counter_to_defultdict(self.model)
        self.sorted_model = dict(self.model.most_common())

            
                
        
        
            
            

    def get_model(self):
        """Returns the model as a dictionary of the form {ngram:count}
        """
        return self.model_defultdict

    def generate(self, context=None, n=20):
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context should be sampled
        from the models' contexts distribution. Generation should stop before the n'th word if the
        contexts are exhausted.
        
        
        !! conext can be only n-1 we assume we live in world of n-1 contexts

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """
        genrated_text = []
        
        if context:
            
            context = context.lower()
            if self.split_by_char:
                context = list(context)
            else:
                context = context.split(" ")
            genrated_text = context
           
        else: #: context dosent exist
            genrated_text = random.choices(list(self.model.keys()), weights=list(self.model.values()), k=1)[0].split(" ")
        
        if self.n_grams == 1:
            while(len(genrated_text) < n and self.generate_bool):
                random_choise = random.choices(list(self.model.keys()), weights=list(self.model.values()), k=1)[0].split(" ")[0]
                genrated_text.append(random_choise)
        else:
            while(len(genrated_text) < n and self.generate_bool):
                self.search_next_phrase(genrated_text)
        
        if self.split_by_char:
            genrated_text_string =  "".join(genrated_text)
        else:
            genrated_text_string =  " ".join(genrated_text)
        
        return genrated_text_string  

    def evaluate(self,text):
        """Returns the log-likelihod of the specified text to be generated by the model.
           Laplace smoothing should be applied if necessary.

           Args:
               text (str): Text to ebaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        
        if self.split_by_char:
            text = list(text)
        else:
            text = text.split(" ")
        
        # self.add_special_char(text)
        
        smooth = False
        likelihod = 1
        count_n_min_1_gram = 0
        count_n_gram = 0
        for first_window_index in range(len(text)-self.n_grams+1):
            words = text[first_window_index:first_window_index+self.n_grams]
            count_n_gram = self.model[self.join_note.join(words)]
            
            if count_n_gram == 0:
                smooth = True
            
            if smooth:
                likelihod *= self.smooth(words)
            else:
                if self.n_grams >1:
                    count_n_min_1_gram = self.model_n_min_1[self.join_note.join(words[:-1])]
                else:
                    count_n_min_1_gram = self.total_words_in_corpos
                likelihod *= (count_n_gram/count_n_min_1_gram)

            
        return math.log(likelihod)

    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """
        count_n_gram = self.model[self.join_note.join(ngram)] +1
        count_n_min_1_gram = self.model_n_min_1[self.join_note.join(ngram[:-1])] + len(self.model)

        smoothed_probability = (count_n_gram/count_n_min_1_gram)
        
        return smoothed_probability
        
    def search_next_phrase(self,phrase):
        """Returns the smoothed (Laplace) probability of the specified ngram.
            
            check the next word distribution from the model.
            
            in addition we check if we have oov more then once to see if the model
            exhausted.
            
            Args:
                phrase (str): the phrase we need to check for calculate next phrase

  
        """
        distribution_dict = {}
        
        
        phrase_part = phrase[-self.n_grams+1:]
        
        for key,value in self.model.items():
            list_key = key.split(self.join_note)
            if self.join_note.join(list_key).startswith(self.join_note.join(phrase_part)):
                distribution_dict[list_key[-1]] = value
        if distribution_dict: #: found next word
            phrase.append(random.choices(list(distribution_dict.keys()), weights=list(distribution_dict.values()), k=1)[0])
            return
        
        #: check if model exhausted by the idea of having only 1 oov in the begging of the context
        phrase.extend(random.choices(list(self.model.keys()), weights=list(self.model.values()), k=1)[0].split(" "))
        if len(phrase) > (self.n_grams -1):
            self.generate_bool = False
        
        
    # def add_special_char(self,text):
    #     #: add n-1 <S> in the beggining for the evaluate smooth
    #     for _ in range(self.n_grams - 1):
    #         text.insert(0,self.special_word)
            
        
            




            
            
def normalize_text(text):
    """Returns a normalized string based on the specifiy string.
       You can add default parameters as you like (they should have default values!)
       You should explain your decitions in the header of the function.
       
       I transform all the sentece into lower case and padding the "." to
       have one space exectly for each side

       Args:
           text (str): the text to normalize

       Returns:
           string. the normalized text.
    """
    text = text.lower()
    padding_text = re.sub('([.,!?()])', r' \1 ', text)
    padding_text = re.sub('\s{2,}', ' ', padding_text)
    return padding_text


def counter_to_defultdict(counter):
    """get counter object and transform it to defultdict

       Args:
           counter (counter): model 

       Returns:
           defultdict. model
    """
    d = defaultdict(int)
    for key,value in counter.items():
        d[key] = value
    return d


def who_am_i():
    """Returns a ductionary with your name, id number and email. keys=['name', 'id','email']
        Make sure you return your own info!
    """
    return {'name': 'or oxenberg', 'id': '312460132', 'email': 'orox@post.bgu.ac.il'}
