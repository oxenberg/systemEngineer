import re
from collections import Counter,defaultdict,namedtuple
from collections import deque
import random
import math
import operator
class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns amodel from a given text.
        It supoprts language generation and the evaluation of a given string.
        The class can be applied on both word level and caracter level.
    """

    def __init__(self, n=3, chars=False):
        """Initializing a language model object.
        Arges:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.n_grams = n
        self.split_by_char = chars
        self.total_words_in_corpos = 0
        self.model =  Counter()
        self.model_n_min_1 =  Counter()
        self.sorted_model = {}
        self.join_note = " "
        # self.special_word = "<S>"
        self.oov = True
        self.generate_bool = True
    def build_model(self, text):  #should be called build_model
        """populates a dictionary counting all ngrams in the specified text.
        
            Args:
                text (str): the text to construct the model from.
        """
   
        
        if self.split_by_char:
            list_of_words = list(text)
            self.join_note = "#@"
        else:
            list_of_words = normalize_text(text).split(" ")
            list_of_words = list_of_words[:-1]
        
        
               
        self.total_words_in_corpos = len(list_of_words) #: for normalization
        
        #: add <s> in the begging for calculate window easy
        # self.add_special_char(list_of_words)
        
        if self.n_grams == 1: #: easy case uni-gram
            self.model = Counter(list_of_words)
        else:
            for first_window_index in range(len(list_of_words) - self.n_grams+1):
                word_window = list_of_words[first_window_index:first_window_index+self.n_grams]
                string_window = self.join_note.join(word_window)
                self.model[string_window] += 1
            for first_window_index in range(len(list_of_words) - (self.n_grams)):
                word_window = list_of_words[first_window_index:first_window_index+(self.n_grams-1)]
                string_window = self.join_note.join(word_window)
                self.model_n_min_1[string_window] += 1
        
        
        
        #: i need to create dict witout the <s> sign to get "same model as output"
        # self.model_without_special_word = defaultdict(int)
        
        # for key,value in self.model.items():
        #     if self.special_word not in key:
        #         self.model_without_special_word[key] = value
        
        self.model_defultdict = counter_to_defultdict(self.model)
        self.sorted_model = dict(self.model.most_common())

            
                
        
        
            
            

    def get_model(self):
        """Returns the model as a dictionary of the form {ngram:count}
        """
        return self.model_defultdict

    def generate(self, context=None, n=20):
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context should be sampled
        from the models' contexts distribution. Generation should stop before the n'th word if the
        contexts are exhausted.
        
        
        !! conext can be only n-1 we assume we live in world of n-1 contexts

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """
        genrated_text = []
        
        if context:
            
            context = context.lower()
            if self.split_by_char:
                context = list(context)
            else:
                context = context.split(" ")
            genrated_text = context
           
        else: #: context dosent exist
            genrated_text = random.choices(list(self.model.keys()), weights=list(self.model.values()), k=1)[0].split(" ")
        
        if self.n_grams == 1:
            while(len(genrated_text) < n and self.generate_bool):
                random_choise = random.choices(list(self.model.keys()), weights=list(self.model.values()), k=1)[0].split(" ")[0]
                genrated_text.append(random_choise)
        else:
            while(len(genrated_text) < n and self.generate_bool):
                self.search_next_phrase(genrated_text)
        
        if self.split_by_char:
            genrated_text_string =  "".join(genrated_text)
        else:
            genrated_text_string =  " ".join(genrated_text)
        
        return genrated_text_string  

    def evaluate(self,text):
        """Returns the log-likelihod of the specified text to be generated by the model.
           Laplace smoothing should be applied if necessary.

           Args:
               text (str): Text to ebaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        
        if self.split_by_char:
            text = list(text)
        else:
            text = text.split(" ")
        
        # self.add_special_char(text)
        
        smooth = False
        likelihod = 1
        count_n_min_1_gram = 0
        count_n_gram = 0
        for first_window_index in range(len(text)-self.n_grams+1):
            words = text[first_window_index:first_window_index+self.n_grams]
            count_n_gram = self.model[self.join_note.join(words)]
            
            if count_n_gram == 0:
                smooth = True
            
            if smooth:
                likelihod *= self.smooth(words)
            else:
                if self.n_grams >1:
                    count_n_min_1_gram = self.model_n_min_1[self.join_note.join(words[:-1])]
                else:
                    count_n_min_1_gram = self.total_words_in_corpos
                likelihod *= (count_n_gram/count_n_min_1_gram)

            
        return math.log(likelihod)

    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """
        count_n_gram = self.model[self.join_note.join(ngram)] +1
        count_n_min_1_gram = self.model_n_min_1[self.join_note.join(ngram[:-1])] + len(self.model)

        smoothed_probability = (count_n_gram/count_n_min_1_gram)
        
        return smoothed_probability
        
    def search_next_phrase(self,phrase):
        """Returns the smoothed (Laplace) probability of the specified ngram.
            
            check the next word distribution from the model.
            
            in addition we check if we have oov more then once to see if the model
            exhausted.
            
            Args:
                phrase (str): the phrase we need to check for calculate next phrase

  
        """
        distribution_dict = {}
        
        
        phrase_part = phrase[-self.n_grams+1:]
        
        for key,value in self.model.items():
            list_key = key.split(self.join_note)
            if self.join_note.join(list_key).startswith(self.join_note.join(phrase_part)):
                distribution_dict[list_key[-1]] = value
        if distribution_dict: #: found next word
            phrase.append(random.choices(list(distribution_dict.keys()), weights=list(distribution_dict.values()), k=1)[0])
            return
        
        #: check if model exhausted by the idea of having only 1 oov in the begging of the context
        phrase.extend(random.choices(list(self.model.keys()), weights=list(self.model.values()), k=1)[0].split(" "))
        if len(phrase) > (self.n_grams -1):
            self.generate_bool = False
        
        
    # def add_special_char(self,text):
    #     #: add n-1 <S> in the beggining for the evaluate smooth
    #     for _ in range(self.n_grams - 1):
    #         text.insert(0,self.special_word)
            
        
            




class Spell_Checker:
    """The class implements a context sensitive spell checker. The corrections
        are done in the Noisy Channel framework, based on a language model and
        an error distribution model.
    """
    def __init__(self,lm = None):
        """Initializing a spell checker object with a language model as an
        instance  variable. The language model should suppport the evaluate()
        and the get_model() functions as defined in assignment #1.

        Args:
            lm: a language model object. Defaults to None
        """
        if lm:
            self.lm = lm
        else:
            self.lm = Ngram_Language_Model()
        
        self.alpha = 0.95
    def build_model(self, text, n=3):
        """Returns a language model object built on the specified text. The language
            model should support evaluate() and the get_model() functions as defined
            in assignment #1.

            Args:
                text (str): the text to construct the model from.
                n (int): the order of the n-gram model (defaults to 3).

            Returns:
                A language model object
        """
        self.lm.n_grams = n #: set n for the model
        self.lm.build_model(text)
        return self.lm
    
    def add_language_model(self,lm):
        """Adds the specified language model as an instance variable.
            (Replaces an older LM disctionary if set)
    
            Args:
                ls: a language model object
        """
        self.lm = lm  
    
    def learn_error_tables(self,error_file):
        """Returns a nested dictionary {str:dict} where str is in:
                <'deletion', 'insertion', 'transposition', 'substitution'> and the
                inner dict {str: int} represents the confution matrix of the
                specific errors, where str is a string of two characters mattching the
                row and culumn "indixes" in the relevant confusion matrix and the int is the
                observed count of such an error (computed from the specified errors file).
                Examples of such string are 'xy', for deletion of a 'y'
                after an 'x', insertion of a 'y' after an 'x'  and substitution
                of 'x' (incorrect) by a 'y'; and example of a transposition is 'xy' indicates the characters that are transposed.
    
    
                Notes:
                    1. Ultimately, one can use only 'deletion' and 'insertion' and have
                        'substitution' and 'transposition' derived. Again,  we use all
                        four types explicitly in order to keep things simple.
                Args:
                    errors_file (str): full path to the errors file. File format, TSV:
                                        <error>    <correct>
    
    
                Returns:
                    A dictionary of confusion "matrices" by error type (dict).
            """
        self.error_table = self.createDict()

        #: read file
        f = open(error_file, 'r')
        for line in f:
            
            # Split on any whitespace (including tab characters)
            row = line.split()
            # Convert strings to numeric values:
            wrong = row[0]
            correct = row[1]
            #create error table for calculate distnce operators
            table = EditDist(wrong,correct) #:toAdd : transposition implamantation.
            table.setErrorWordTable(self.error_table)
            table.fillTable()
            self.error_table = table.getOpeartions()
            # Append to our list of lists:
        f.close()
        return self.error_table
    def add_error_tables(self, error_tables):
        """ Adds the speficied dictionary of error tables as an instance variable.
            (Replaces an older value disctionary if set)

            Args:
                error_tables (dict): a dictionary of error tables in the format
                returned by  learn_error_tables()
        """
        self.error_table = error_tables



    def evaluate(self,text):
        """Returns the log-likelihod of the specified text given the language
            model in use. Smoothing is applied on texts containing OOV words

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        return self.lm.evaluate(text)
    def spell_check(self,text,alpha):
        """ Returns the most probable fix for the specified text. Use a simple
             noisy channel model if the number of tokens in the specified text is
             smaller than the length (n) of the language model.
             
             ###added by me###
             I assume that the model I got is word based model and not a char.
             this assampteion is reasonable becouse the error table is word base
             
        
             Args:
                 text (str): the text to spell check.
                 alpha (float): the probability of keeping a lexical word as is.
        
             Return:
                 A modified string (or a copy of the original if no corrections are made.)
        """
        self.alpha = alpha
        overallWordCount = 0
        simpleModleMode = False
        text = normalize_text(text).split()
        textLength = len(text)
        #: create unique words and word dict with count
        wordDict = self.lm.get_model()
        allWords = set([key.split()[0] for key in wordDict.keys()])
        N = len(next(iter(wordDict)).split())
        
        #: 
        if N > textLength:
            newText = " ".join([item for key,count in wordDict.items() for item in [key]*count])
            oldModel  = self.lm
            self.add_language_model(Ngram_Language_Model(n =1))
            self.lm.build_model(newText)
            N = 1
            simpleModleMode = True 
        
        countWords = Counter()
        for key,value in wordDict.items():
            countWords["#"+key.split()[0]]+=value
            overallWordCount+=value
            
        #: create related work for each word
        relatedWordsDict = {}
        graph = editGraph(allWords)
        # calculate Probabilty Noise Channel by fill dict
        denominatorCounter = Counter() 
        probabiltyDict = self.createDict()
        
       
        tableCorrection = []
        for wordIndes,word in enumerate(text):
            if word == ".":
                continue
            
            
            if word not in relatedWordsDict:
                relatedWordsDict[word] = graph.getAllRelatedWords(word)
        
            correctWords = relatedWordsDict[word]
            if word in correctWords:
                correctWords.remove(word)
                

            #: check if there is not any word to replace
            if len(correctWords) == 0:
                continue
            
            #: calculate index in text to evalaute
            minIndex = max(0,wordIndes - N -1 )
            maxIndex = min(textLength,wordIndes+ N)
            
            ProbabiltyNoiseChannelSum = 0
            
            chosenWord = {"index" : wordIndes,"correctWord" : word,"modelErrorValue" : -math.inf}
            
        
            #run over all possible replacment
                       
            for correctWord in correctWords:
                textReplaced = text[:]
                textReplaced[wordIndes] = correctWord
                
                ProbabiltyNoiseChannel = self.fillProbabiltyNoiseChannelDict(probabiltyDict,correctWord,word,countWords,denominatorCounter) 
                ProbabiltyNoiseChannelSum += ProbabiltyNoiseChannel
                
                if ProbabiltyNoiseChannel == 0 :
                    continue
                
                if (maxIndex - minIndex) >= N:
                    ProbabiltyLengModel = math.exp(self.lm.evaluate(" ".join(textReplaced[minIndex:maxIndex])))
                else:
                    ProbabiltyLengModel = 1
                modelErrorValue = ProbabiltyNoiseChannel*ProbabiltyLengModel
                # print(word , correctWord,modelErrorValue) #debug
                if chosenWord["modelErrorValue"] < modelErrorValue:
                    chosenWord["correctWord"] = correctWord
                    chosenWord["modelErrorValue"] = modelErrorValue
                    chosenWord["ProbabiltyNoiseChannel"] = ProbabiltyNoiseChannel
            #: normalize by 1 -alpha
            if ProbabiltyNoiseChannelSum == 0:
                chosenWord["modelErrorValue"] = 0
            else: 
                chosenWord["modelErrorValue"] /= (ProbabiltyNoiseChannelSum /(1- self.alpha))
            
            #: compere to dont replace the word
            
            if (maxIndex - minIndex) >= N:
                    ProbabiltyLengModel = math.exp(self.lm.evaluate(" ".join(text[minIndex:maxIndex])))
            else:
                   ProbabiltyLengModel = 1
            modelErrorValue = self.alpha * ProbabiltyLengModel

            # print(word , word,modelErrorValue) #debug
            #: the best replacement for this word is itself :)
            if chosenWord["modelErrorValue"] < modelErrorValue:
                 chosenWord["index"] = -1
                 chosenWord["correctWord"] = word
                 chosenWord["modelErrorValue"] = modelErrorValue
                 chosenWord["ProbabiltyNoiseChannel"] = ProbabiltyNoiseChannel
                
                
           
            tableCorrection.append(chosenWord)
        
        
        #final words score
        bestString = ["",-math.inf]
        # print(tableCorrection)
        for wordRaw in tableCorrection:
            textReplaced = text[:]
            if wordRaw["index"] > -1:
                textReplaced[wordRaw["index"]] = wordRaw["correctWord"]

            ProbabiltyLengModel = math.exp(self.lm.evaluate(" ".join(textReplaced)))
            modelErrorValueFinal = ProbabiltyLengModel* wordRaw["ProbabiltyNoiseChannel"]
            if bestString[1] < modelErrorValueFinal:
                bestString[0] = textReplaced
                bestString[1]  = modelErrorValueFinal
                
        if simpleModleMode:
            self.add_language_model = oldModel

        return " ".join(bestString[0])
            
                 
            
                   
            
                       
                       
    #### added function to class by me
    def fillProbabiltyNoiseChannelDict(self,probabiltyDict,correct,wrong,countWords,denominatorCounter):
        """ Returns the ProbabiltyNoiseChannel for 2 words 
             Args:
                 probabiltyDict (dict): probabilty NoiseChannel for each word.
                 correct (str):correct word.
                 wrong (str):wrong word.
                 countWords (dict): occurnce for each word
                 denominatorCounter (dict): saved denominator for each chars and error type

        
             Return:
                Returns the ProbabiltyNoiseChannel
        """
        listOfError = [] #: list of tuple (errorchars,errorType)

        
        #collect char errors
        table = EditDist("#"+wrong,"#" + correct)
        table.fillTable()
        dictErrorType = table.getOpeartions()
        for errorType,charsCountDict in dictErrorType.items():
            for errorChars,value in charsCountDict.items():
                listOfError.extend([(errorChars,errorType)] * value)
                
        
        normalizeDict = {"substitution" : lambda word,chars : word.count(chars[1]),
                         "insertion" : lambda word,chars : word.count(chars[0]),
                         "deletion" : lambda word,chars : word.count(chars),
                         "transposition" : lambda word,chars : word.count(chars)}
        
        probabiltyAll = 1
        for errorChars,errorType in listOfError:
            #: check if this error type and chars were before
            if errorChars not in probabiltyDict[errorType]:
                #calculate the nominator
                try:
                    nominator = self.error_table[errorType][errorChars]
                except KeyError: 
                    nominator = 0
                #calculate the denominator
                if denominatorCounter[(errorChars,errorType)] == 0:
                    denominator = 0
                    for word,count in countWords.items():
                        denominator += normalizeDict[errorType](word,errorChars) * count
                    if denominator == 0:
                        print("error") #debug
                    denominatorCounter[(errorChars,errorType)] = denominator
                probabilty = nominator/denominator
                
                probabiltyDict[errorType][errorChars] = probabilty
            probabiltyAll*= probabiltyDict[errorType][errorChars]
            
            
        return probabiltyAll
    
    def createDict(self):
        """ init error table scheme
            
        
             Return:
               error empty table
        """
        error_table = {}
        error_table["substitution"] = Counter()
        error_table["insertion"] = Counter()
        error_table["deletion"] = Counter()
        error_table["transposition"] = Counter()
        return error_table
            




class editGraph():
    """ collect all the words for some edit distance (1 and 2)
            
    """
    def __init__(self,allWords):
        """ init editGraph
            Args:
                 allWords (set): all posible words.

   
        """
        self.allWords = allWords
    def edits1(self,word):
        """ All edits that are one edit away from `word`.
            Args:
                 word (str): word to calcous distance from

           return:
               All edits that are one edit away from `word`
        """
        letters    = 'abcdefghijklmnopqrstuvwxyz'
        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]
        deletes    = [L + R[1:]               for L, R in splits if R]
        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]
        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]
        inserts    = [L + c + R               for L, R in splits for c in letters]
        return set(deletes + transposes + replaces + inserts)
    
    def edits2(self,words): 
        """ All edits that are two edit away from `word`.
            Args:
                 word (str): word to calcous distance from

           returns:
               All edits that are one edit away from `word`
        """
        return (e2 for e1 in words for e2 in self.edits1(e1))
    def known(self,words): 
        """ 
            Args:
                 word (str): word to calcous distance from

           returns:
               All words in allword set 
        """
        return set(w for w in words if w in self.allWords)
    def getAllRelatedWords(self,word):
        """ 
            Args:
                 word (str): word to calcous distance from

           returns:
               set of all the distance 1 and 2 that are in the word space
        """
        wordsDist1 = self.edits1(word)
        wordsDist2 = self.edits2(wordsDist1)
        allRelatedWords = wordsDist1.union(wordsDist2)
        return self.known(allRelatedWords)
        

        


#: toAdd create graph class to get all word from length 2
#: spell_checker need to 




class EditDist():
    """ get the path of the edit distance and the error dict
    """
    def __init__(self,wrong,correct):
        """ 
            Args:
                 wrong (str): wrong word
                 correct (str) : correct word

        """
        self.correct = correct
        self.wrong = wrong
        self.lenCorrect = len(correct) + 1
        self.lenWrong = len(wrong) + 1
        self.distTable = self.initTable(self.lenWrong,self.lenCorrect)
        self.initErrorTable()
        
    def initErrorTable(self):
        """ 
            init empty error table 
        """
        self.errorWordTable = {}
        self.errorWordTable["substitution"] = Counter()
        self.errorWordTable["insertion"] = Counter()
        self.errorWordTable["deletion"] = Counter()
        self.errorWordTable["transposition"] = Counter()
        
    def initTable(self,n,m):
        """ init zeros and 1 to n values matrix
            Args:
                 n (str):len Of wrong
                 m (str):len Of right


           returns:
               init table as list of lists
        """
        distTable = []
        for i in range(n):
            distTable.append([0] * m)  # initialize 2D array to zero
        for i in range(n):
            distTable[i][0] = i  # column 0: 0,1,2,3,4,...
        for j in range(m):
            distTable[0][j] = j  # row 0: 0,1,2,3,4,...
        return distTable
    
    
    def setErrorWordTable(self,errorWordTable):
        """ set the error word table for the class

        """
        self.errorWordTable = errorWordTable
        
    def fillTable(self):
        """ Calculate the Levenshtein edit-distance with dynamic programing
        
        """
        for wrong_index in range(1,self.lenWrong):
            for correct_index in range(1,self.lenCorrect):
                self.distTable[wrong_index][correct_index] = min([
                    self.distTable[wrong_index-1][correct_index] + 1, #delete
                    self.distTable[wrong_index][correct_index-1] + 1, #insertion
                    self.distTable[wrong_index-1][correct_index-1]+( 1 if self.correct[correct_index-1] !=self.wrong[wrong_index-1] else 0), #substitution
                    ])
                # t = self.distTable #too tests
    def getReversedPath(self):
        """ Calculate the minimum Levenshtein edit-distance and get the path



           returns:
               path as list of tuple of indexs and action
        """
        currentPoint = ((self.lenWrong-1,self.lenCorrect-1),"n")
        path = deque([currentPoint])
        
        while currentPoint[0] != (0,0):
            point,_ = currentPoint
            insertion = (point[0]-1,point[1],"insertion")
            sub = (point[0] - 1, point[1] -1,"substitution" )
            deletion = (point[0],point[1]-1,"deletion")
            
            directions = [sub,deletion,insertion]
            
            direction_costs = (
            (self.distTable[i][j] if (i >= 0 and j >= 0) else float("inf"), (i, j),opeartion)
            for i, j,opeartion in directions
        )
            value,point,op = min(direction_costs, key=operator.itemgetter(0))
            
            if op == "substitution":
                if value != self.distTable[path[0][0][0]][path[0][0][1]]:
                    path[0] = (path[0][0],"substitution")
            else:
                path[0] = (path[0][0],op)
            op = "n"


            currentPoint = (point,op)
            path.appendleft(currentPoint)
        return path
    def printTable(self):
        """ print the matrix of distances

        """
        prettyDistTable = self.distTable.copy()
        distTable = [list(self.correct)] + prettyDistTable
        print(distTable)
        return distTable
    def getOpeartions(self):
        """ get the error table as a dict
        
           returns
               error table as a dict
        """

        allChards = []
        path = self.getReversedPath()
        
        countTransposition = 0
        for axis,op in path:
            correctIndex = axis[1] - 1
            wrongIndex = axis[0] - 1
            #: no need to check' no change
            if op == 'n':
                continue
            
            if op == 'substitution':
                countTransposition+=1
                firstChar = self.wrong[wrongIndex] 
                secondChar = self.correct[correctIndex]

                chars = f"{firstChar}{secondChar}"
                #only 2 substitution in a raw
                if countTransposition >=2:
                    beforeChars = allChards[len(allChards)-1]
                    beforeCharsOpiste = beforeChars[::-1]
                    
                    #found transposition
                    if chars == beforeCharsOpiste:
                        self.errorWordTable["transposition"][chars] +=1
                        self.errorWordTable[op][beforeChars] -=1
                        continue
                        #: we found transposition so we changed the before chars to -1 
                        #: and add +1 with this spefic transposition
                
                self.errorWordTable[op][chars] +=1
            else:
                countTransposition = 0
                if op == 'insertion':
                    try:
                        firstChar = self.correct[correctIndex]
                    except:
                        firstChar = self.wrong[wrongIndex-1]
                    if self.wrong == "paralell":
                        print("h")
                    secondChar = self.wrong[wrongIndex]
                    
                else:
                    firstChar = self.correct[correctIndex-1]
                    secondChar = self.correct[correctIndex]
                
                chars = f"{firstChar}{secondChar}"
                self.errorWordTable[op][chars] +=1
            allChards.append(chars)

        return self.errorWordTable

def counter_to_defultdict(counter):
    """get counter object and transform it to defultdict

       Args:
           counter (counter): model 

       Returns:
           defultdict. model
    """
    d = defaultdict(int)
    for key,value in counter.items():
        d[key] = value
    return d

def normalize_text(text):
    """Returns a normalized string based on the specifiy string.
       You can add default parameters as you like (they should have default values!)
       You should explain your decitions in the header of the function.
       
       I transform all the sentece into lower case and padding the "." to
       have one space exectly for each side

       Args:
           text (str): the text to normalize

       Returns:
           string. the normalized text.
    """
    text = text.lower()
    padding_text = re.sub('([.,!?()])', r' \1 ', text)
    padding_text = re.sub('\s{2,}', ' ', padding_text)
    return padding_text





def who_am_i(): #this is not a class method
    """Returns a ductionary with your name, id number and email. keys=['name', 'id','email']
        Make sure you return your own info!
    """
    return {'name': 'or oxenberg', 'id': '312460132', 'email': 'orox@post.bgu.ac.il'}











