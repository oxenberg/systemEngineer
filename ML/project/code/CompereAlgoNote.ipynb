{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on RF_baseClassfier..\n",
      "--------------------------------\n",
      "cv num 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oxenb\\Anaconda3\\envs\\ml-project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv num 2\n",
      "cv num 3\n",
      "cv num 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oxenb\\Anaconda3\\envs\\ml-project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv num 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oxenb\\Anaconda3\\envs\\ml-project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv num 6\n",
      "cv num 7\n",
      "cv num 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oxenb\\Anaconda3\\envs\\ml-project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv num 9\n",
      "cv num 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [02:16<00:00, 136.84s/it]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import json\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,KFold,RandomizedSearchCV,StratifiedShuffleSplit,ShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,roc_curve,precision_score,average_precision_score\n",
    "from sklearn.datasets import fetch_covtype, load_svmlight_file\n",
    "from sklearn.base import clone\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "#ktboost model\n",
    "import KTBoost.KTBoost as KTBoost\n",
    "\n",
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import k_categorical\n",
    "\n",
    "from logitboost import LogitBoost\n",
    "\n",
    "\n",
    "global INSTANCES_TO_MEASURE_TIME\n",
    "INSTANCES_TO_MEASURE_TIME = 1000\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "sys.path.append(\"../infiniteboost/research/\")\n",
    "from SparseInfiniteBoosting import InfiniteBoosting\n",
    "\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "def getsDataPaths():\n",
    "    \"\"\"get all the datasets paths for import\n",
    "    \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        allDataSetsPaths : {list}\n",
    "            all the paths to the datasets\n",
    "            \n",
    "    \"\"\"\n",
    "    allDataSetsPaths = []\n",
    "    dataSetName = \"classification_datasets\"\n",
    "    dataPath = \"../data\"\n",
    "    for file in os.listdir(f\"{dataPath}/{dataSetName}\"):\n",
    "        if file.endswith(\".csv\"):\n",
    "            path = os.path.join(f\"{dataPath}/{dataSetName}\", file)\n",
    "            allDataSetsPaths.append(path)\n",
    "            \n",
    "    return allDataSetsPaths\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "def setClassColumn(data,names):\n",
    "    columns = data.columns\n",
    "    indexOfClass = np.where(columns.str.lower().isin(names) == True)\n",
    "    if len(indexOfClass[0]) == 0:\n",
    "        return\n",
    "    columns = list(columns)\n",
    "    pos1 = indexOfClass[0][0]\n",
    "    pos2 = len(columns)-1\n",
    "    data[columns[pos1]], data[columns[pos2]] = data[columns[pos2]], data[columns[pos1]] \n",
    "    columns[pos1], columns[pos2] = columns[pos2], columns[pos1] \n",
    "    data.columns = columns\n",
    "\n",
    "def getBadLabel(data,TH = 9):\n",
    "    \"\"\"Find the labels with less then some threshold of instances\n",
    "    \n",
    "        ----------\n",
    "        data : {dataframe} of shape (n_samples, n_features)\n",
    "        \n",
    "        TH : {int} the minimun value of instances with same label\n",
    "        for exemple if we have less then TH rows with same label we will consider\n",
    "        it bad label\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        badLabels : {indexs}\n",
    "            all the labels that answer the condtion\n",
    "            \n",
    "    \"\"\"\n",
    "    countSeries = data.iloc[:,-1].value_counts()\n",
    "    badLabels = countSeries[countSeries< TH].index\n",
    "    return badLabels\n",
    "\n",
    "def preprocess(path):\n",
    "    \"\"\"preprocess the datasets. read each dataset,impute missing values and convert categorial columns\n",
    "    \n",
    "        ----------\n",
    "        path : {string} path to the dataset\n",
    "        \n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        X: {dataframe} of shape (n_samples, n_features)\n",
    "        y: {dataframe} of shape (n_samples, 1) - label column\n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "    TH = 10\n",
    "\n",
    "    #check if we have class column in data\n",
    "    setClassColumn(data,[\"class\"])\n",
    "        \n",
    "    imp_frq = SimpleImputer(strategy='most_frequent')\n",
    "    imp_frq.fit(data)\n",
    "    data = pd.DataFrame( imp_frq.transform(data), columns =data.columns)\n",
    "    \n",
    "    data = data.dropna()\n",
    "    \n",
    "    dataSetName = path.split(\"\\\\\")[1].split(\".\")[0]\n",
    "    if dataSetName == \"lenses\":\n",
    "        TH = 9\n",
    "    badLabels = getBadLabel(data,TH)\n",
    "    data.iloc[:,-1] = data.iloc[:,-1].apply(lambda x : \"other\" if x in list(badLabels) else x)\n",
    "\n",
    "    #check if we still have bad lables under the TH\n",
    "    \n",
    "        \n",
    "    badLabels = getBadLabel(data,TH)\n",
    "    data = data[~data.iloc[:,-1].isin(badLabels)]\n",
    "\n",
    "\n",
    "    strCoulmns = list(data.dtypes[data.dtypes == \"object\"].index)\n",
    "    labelColumn = [list(data.columns)[-1]]\n",
    "    if labelColumn not in strCoulmns:\n",
    "        strCoulmns+=labelColumn\n",
    "    if len(strCoulmns) > 0:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        for i in strCoulmns:\n",
    "            data[i] = data[i].astype('str')\n",
    "            data[i] = le.fit_transform(data[i])\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:,-1]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def calcFitTime(RS_model, X_train, y_train):\n",
    "    \"\"\"calculate the time to train the model\n",
    "\n",
    "        ----------\n",
    "        RS_model : {sklearn model, other model with fit method} the model\n",
    "        X_train,y_train : {array-like}  of shape (n_samples, n_features)\n",
    "\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        T_time: {datetime} \n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now()\n",
    "    RS_model.fit(X_train, y_train)\n",
    "    time_stop = datetime.datetime.now()\n",
    "    T_time = time_stop- now ##for time calc of training\n",
    "    return T_time\n",
    "\n",
    "def calcInferenceTime(X_test, RS_model):\n",
    "    \"\"\"calculate the time to test 1000 rows from the dataset the model and calculate the predict\n",
    "       values\n",
    "\n",
    "        ----------\n",
    "        RS_model : {sklearn model, other model with fit method} the model\n",
    "        X_test : {array-like}  of shape (n_samples, n_features)\n",
    "\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        inferenceTime: {datetime} \n",
    "        y_pred: {array-like} predict values\n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "    instancesOfTest = len(X_test)\n",
    "    if (instancesOfTest) < INSTANCES_TO_MEASURE_TIME:\n",
    "        now = datetime.datetime.now()\n",
    "        y_pred = RS_model.predict(X_test)\n",
    "        time_stop = datetime.datetime.now()\n",
    "        inferenceTime = (time_stop- now) *(INSTANCES_TO_MEASURE_TIME/instancesOfTest)\n",
    "    else:\n",
    "        indexSample = random.sample(list(np.arange(len(X_test))), INSTANCES_TO_MEASURE_TIME)\n",
    "        X_test_sample = X_test[indexSample]\n",
    "        now = datetime.datetime.now()\n",
    "        RS_model.predict(X_test_sample)\n",
    "        time_stop = datetime.datetime.now()\n",
    "        inferenceTime = time_stop- now\n",
    "        y_pred = RS_model.predict(X_test)\n",
    "    \n",
    "    return inferenceTime, y_pred\n",
    "\n",
    "def calcFprTpr(y_test, y_pred_proba, classes):\n",
    "    \"\"\"calculate the fpr and tpr with the macro average method\n",
    "\n",
    "        ----------\n",
    "        y_pred_proba,y_test : {array-like}  of shape (n_samples, n_features)\n",
    "        classes : {int} amount of classes for the dataset\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        inferenceTime: {datetime} \n",
    "        y_pred: {array-like} predict values\n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    \n",
    "    for i,class_ in enumerate(classes):\n",
    "        fpr[class_], tpr[class_], _ = roc_curve(y_test == class_, y_pred_proba[:, i])\n",
    "    \n",
    "    all_fpr = np.unique(np.concatenate([fpr[class_] for class_ in classes]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for class_ in classes:\n",
    "        mean_tpr += np.interp(all_fpr, fpr[class_], tpr[class_])\n",
    "    \n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= len(classes)\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    return tpr[\"macro\"], fpr[\"macro\"]\n",
    "\n",
    "def calcAPS(y_test,y_pred_proba):\n",
    "    \"\"\"calculate the average precison score (area uneder the precison recall curve)\n",
    "\n",
    "        ----------\n",
    "        y_pred_proba,y_test : {array-like}  of shape (n_samples, n_features)\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        APS: {array-like} \n",
    "            \n",
    "    \"\"\"\n",
    "    #trnasform y to mulitly label format (vector) for average_precision_score func\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    multiyY = enc.fit_transform(y_test.reshape(-1,1)).toarray()\n",
    "    APS = average_precision_score(multiyY,y_pred_proba, average='macro')\n",
    "    return APS\n",
    "    \n",
    "def clacMeasure(RS_model,X_train,X_test,y_train,y_test, datasetName,AlgoName = None, cvIndex = None, n_classes = 2, multiclass = False):\n",
    "    \"\"\"Calculating mesurements over all the datasets and algorithems\n",
    "    \n",
    "        ----------\n",
    "        RS_model : {sklearn randomsearch model} the model we use \n",
    "        to calc the preformance on\n",
    "        \n",
    "        X_train,X_test,y_train,y_test : {array-like}  of shape (n_samples, n_features)\n",
    "        \n",
    "        datasetName,AlgoName : {string}\n",
    "        \n",
    "        cvIndex,n_classes : {int}\n",
    "        \n",
    "        multiclass : {bool}\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        row : {list} \n",
    "            contain all the needed measuerments\n",
    "            \n",
    "    \"\"\"\n",
    "    row = []\n",
    "    \n",
    "    T_time = calcFitTime(RS_model, X_train, y_train)\n",
    "    \n",
    "    inferenceTime, y_pred = calcInferenceTime(X_test, RS_model)\n",
    "    y_pred_proba = RS_model.predict_proba(X_test)\n",
    "    \n",
    "\n",
    "    #collect measures\n",
    "    row.append(datasetName)\n",
    "    row.append(AlgoName)\n",
    "    row.append(cvIndex)\n",
    "    row.append(str(RS_model.best_params_))\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    row.append(acc)\n",
    "  \n",
    "       \n",
    "    #soultion for multiclass\n",
    "    \n",
    "    tpr, fpr = calcFprTpr(y_test, y_pred_proba,n_classes)\n",
    "    row.append(tpr)\n",
    "    row.append(fpr)\n",
    "    \n",
    "    # calculating precsion\n",
    "    precsion = precision_score(y_test, y_pred, average='macro')\n",
    "    row.append(precsion)\n",
    "    \n",
    "    # calculating AUC\n",
    "    if(multiclass):\n",
    "        AUC = roc_auc_score(y_test, y_pred_proba,multi_class = \"ovr\", average = 'macro')\n",
    "    else:\n",
    "        AUC = roc_auc_score(y_test, y_pred_proba[:,1], average = 'macro')\n",
    "    row.append(AUC)\n",
    "    \n",
    "    APS = calcAPS(y_test,y_pred_proba)\n",
    "    \n",
    "    row.append(APS)\n",
    "    row.append(T_time)\n",
    "    row.append(inferenceTime)\n",
    "    return row\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "def CreateModels():\n",
    "    \"\"\"add models to dict with the name and the modle itself\n",
    "    \n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        row : {dict} \n",
    "            model name, value: {list} contain model and hyperparmater distributions\n",
    "            \n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    models = {}\n",
    "    \n",
    "    distributions = dict(estimator__n_estimators=np.arange(5,20),estimator__max_leaf_nodes = [2,4,3,5,6,7,8,9])\n",
    "    models['infiboost'] = [InfiniteBoosting(),distributions]\n",
    "    \n",
    "    distributions = dict(estimator__max_depth=np.arange(2,8),estimator__n_estimators=np.arange(5,20))\n",
    "    models['KTBoost'] = [KTBoost.BoostingClassifier(),distributions]\n",
    "    \n",
    "    distributions = dict(estimator__n_estimators = np.arange(5,20),estimator__Base__max_depth=np.arange(4,13))\n",
    "    models[\"NGBClassifier\"] = [\"NGB\",distributions]\n",
    "    \n",
    "    distributions = dict(estimator__n_estimators = np.arange(5,20),estimator__max_depth=np.arange(3,15))\n",
    "    models[\"RF_baseClassfier\"] = [RandomForestClassifier(),distributions]\n",
    "    \n",
    "    return models\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def RunModels(models,allDataSetsPaths):\n",
    "    \"\"\"run the model for each dataset and for each algorithm with cv and random search. \n",
    "       create the measure table with the clacMeasure function and export it to file.\n",
    "        \n",
    "    \n",
    "        ----------\n",
    "        models : {dictonary}  key: model name, value: {list} contain model and hyperparmater distributions\n",
    "        \n",
    "        allDataSetsPaths : {list}  all paths to datasets\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "    measuers = pd.DataFrame(columns = [\"Dataset_Name\",\"AlgoName\",\"CrossVal\",\"HP_vals\",\n",
    "                                        \"ACC\",\"TPR\",\"FPR\",\"Precsion\",\"AUC\",\"Precstion_Recall\",\n",
    "                                        \"Training_Time\",\"Inference_Time\"])\n",
    "    measuers.to_csv(\"../data/results/measuers.csv\")\n",
    "    index = 0\n",
    "    #run over all the algo\n",
    "    for AlgoName,items in models.items():\n",
    "        model = items[0]\n",
    "        dist = items[1]\n",
    "        print(f\"run on {AlgoName}..\")\n",
    "        # run over all the datasets\n",
    "        print(\"--------------------------------\")\n",
    "        for datasetName in tqdm(allDataSetsPaths[23:24]):\n",
    "            X, y = preprocess(datasetName)\n",
    "            X = X.values\n",
    "            y = y.values\n",
    "            Dataname = datasetName.split(\"\\\\\")[1].split(\".\")[0]\n",
    "            multiclass = False\n",
    "            if(len(np.unique(y))>2):\n",
    "                    multiclass = True\n",
    "            if Dataname == \"lenses\":\n",
    "                folder = StratifiedShuffleSplit(n_splits=3, random_state=42)\n",
    "            else:\n",
    "                folder = StratifiedShuffleSplit(n_splits=TEST_TRAIN_CV, random_state=42)\n",
    "            cvIndex = 1\n",
    "            # run over train test cv\n",
    "            for train_indices, test_indices in folder.split(X, y):\n",
    "                print(f\"cv num {cvIndex}\")\n",
    "                #create data after cv from indexs\n",
    "                X_train = X[train_indices]\n",
    "                X_test = X[test_indices]\n",
    "                y_train = y[train_indices]\n",
    "                y_test = y[test_indices]\n",
    "                \n",
    "                #the selected model in the iteration of the cv, we need to do 1 vs all for the multi class \n",
    "                # if is binary class we stil can use the 1 vs all becouse is act the same\n",
    "                if model == \"NGB\":\n",
    "                    model = NGBClassifier(Dist=k_categorical(len(np.unique(y))),verbose=False)\n",
    "                clf = OneVsRestClassifier(clone(model))\n",
    "                # we chose for the hyperparm tuning random search\n",
    "                RS = RandomizedSearchCV(clf, dist, random_state=42,n_iter = RANDOM_SEARCH_ITER,cv = TRAIN_VALIDATION_CV)\n",
    "    \n",
    "                #add row to the measuerment table\n",
    "                classes = np.unique(y)\n",
    "                Measure = clacMeasure(RS,X_train,X_test,y_train,y_test, Dataname,AlgoName, cvIndex, classes, multiclass)\n",
    "                measuers.loc[index] = Measure\n",
    "                Measure = pd.DataFrame(Measure).T\n",
    "                Measure.to_csv(\"../data/results/measuers.csv\", mode='a', header=False)\n",
    "                cvIndex+=1\n",
    "                index+=1\n",
    "        #     del clf\n",
    "    \n",
    "    #save the measures\n",
    "    measuers.to_csv(\"../data/results/measuers.csv\")\n",
    "    measuers.to_pickle('../data/results/measuers_pickle.csv')\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "def CompereAlgo(testTrainCV = 10 ,trainValCV = 3,randomSearchIter = 50,debug = False):\n",
    "    \"\"\"main function to this module, get parmeters for training and create the mesures with all \n",
    "       the models \n",
    "        \n",
    "    \n",
    "        ----------\n",
    "        testTrainCV,trainValCV : {int}  CV amount for test-train and train-validation\n",
    "        \n",
    "        randomSearchIter : {int}  iteration number for random search for each model\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "    global INSTANCES_TO_MEASURE_TIME,TEST_TRAIN_CV,TRAIN_VALIDATION_CV,RANDOM_SEARCH_ITER\n",
    "    ##global\n",
    "    TEST_TRAIN_CV = testTrainCV\n",
    "    TRAIN_VALIDATION_CV = trainValCV\n",
    "    RANDOM_SEARCH_ITER = randomSearchIter\n",
    "    \n",
    "    allDataSetsPaths = getsDataPaths()\n",
    "    \n",
    "    ##debug mode\n",
    "    if (debug):\n",
    "        for i,path in enumerate(allDataSetsPaths): \n",
    "            name = path.split('\\\\')[-1].split('.')[0]\n",
    "            X, y = preprocess(path)\n",
    "            labelsAmount = len(y.unique())\n",
    "            if labelsAmount <2:\n",
    "                print (\"----label less then 2 -----\")\n",
    "            if  X.shape[0] <25:\n",
    "                print (\"-------------small size dataset---------------------\")\n",
    "            print ('id', i ,name, 'shape: ', X.shape[0], 'with\\n', y.value_counts(),'labels') \n",
    "    else:\n",
    "        models = CreateModels()\n",
    "        RunModels(models,allDataSetsPaths)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "CompereAlgo(debug = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
